"""
ëª¨ë¸ í‰ê°€(evaluation)ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” Metrics ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ.

- Classical ML (SVM, RF, XGB)
- Deep Learning (MLP, CNN; PyTorch ê¸°ë°˜)

ë‘ ê²½ìš° ëª¨ë‘ y_true / y_predë§Œ ìˆìœ¼ë©´ ë™ì¼í•œ í‰ê°€ í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥.

ì£¼ìš” ê¸°ëŠ¥:
    - accuracy ê³„ì‚°
    - confusion matrix ê³„ì‚°
    - classification report ìƒì„±
    - confusion matrix ì‹œê°í™”(Seaborn)
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ---------------------------------------------------------------------------
# ğŸ”µ Accuracy
# ---------------------------------------------------------------------------


def compute_accuracy(y_true: Iterable[int], y_pred: Iterable[int]) -> float:
    """
    accuracy = ì •í™•íˆ ë§ì¶˜ ë¹„ìœ¨

    Args:
        y_true (Iterable[int]): ì •ë‹µ ë¼ë²¨
        y_pred (Iterable[int]): ì˜ˆì¸¡ ë¼ë²¨

    Returns:
        float: accuracy (0~1)
    """
    return float(accuracy_score(y_true, y_pred))


# ---------------------------------------------------------------------------
# ğŸ”µ Confusion Matrix
# ---------------------------------------------------------------------------


def compute_confusion_matrix(
    y_true: Iterable[int],
    y_pred: Iterable[int],
) -> np.ndarray:
    """
    Confusion matrixë¥¼ numpy arrayë¡œ ë°˜í™˜.

    Args:
        y_true (Iterable[int]): ì •ë‹µ ë¼ë²¨
        y_pred (Iterable[int]): ì˜ˆì¸¡ ë¼ë²¨

    Returns:
        np.ndarray: (C, C) confusion matrix
    """
    return confusion_matrix(y_true, y_pred)


# ---------------------------------------------------------------------------
# ğŸ”µ Classification Report
# ---------------------------------------------------------------------------


def compute_classification_report(
    y_true: Iterable[int],
    y_pred: Iterable[int],
    target_names: Optional[List[str]] = None,
) -> str:
    """
    precision / recall / f1-score ì¶œë ¥.

    Args:
        y_true (Iterable[int]): ì •ë‹µ ë¼ë²¨
        y_pred (Iterable[int]): ì˜ˆì¸¡ ë¼ë²¨
        target_names (List[str], optional): í´ë˜ìŠ¤ ì´ë¦„

    Returns:
        str: classification report
    """
    return classification_report(y_true, y_pred, target_names=target_names)


# ---------------------------------------------------------------------------
# ğŸ”µ Confusion Matrix Visualization
# ---------------------------------------------------------------------------


def save_confusion_matrix_plot(
    cm: np.ndarray,
    labels: List[str],
    save_path: Path,
    title: str = "Confusion Matrix",
    figsize: Tuple[int, int] = (8, 6),
    cmap: str = "Blues",
) -> None:
    """
    Confusion matrixë¥¼ Heatmapìœ¼ë¡œ ì €ì¥í•œë‹¤.

    Args:
        cm (np.ndarray): (C, C) confusion matrix
        labels (List[str]): í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        save_path (Path): ì €ì¥ ê²½ë¡œ
        title (str): ê·¸ë¦¼ ì œëª©
        figsize (Tuple[int,int]): ê·¸ë¦¼ í¬ê¸°
        cmap (str): ìƒ‰ìƒë§µ
    """
    save_path = Path(save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    plt.figure(figsize=figsize)
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap=cmap,
        xticklabels=labels,
        yticklabels=labels,
    )
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


# ---------------------------------------------------------------------------
# ğŸ”µ í†µí•© í‰ê°€ í•¨ìˆ˜ (TrainerBaseì—ì„œ ì‚¬ìš©)
# ---------------------------------------------------------------------------


@dataclass
class MetricResult:
    """
    í‰ê°€ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°.

    Attributes:
        accuracy (float): ì •í™•ë„(0~1)
        confusion_matrix (np.ndarray): confusion matrix
        report (str): classification report ë¬¸ìì—´
    """

    accuracy: float
    confusion_matrix: np.ndarray
    report: str


def evaluate_classification(
    y_true: Iterable[int],
    y_pred: Iterable[int],
    labels: Optional[List[str]] = None,
) -> MetricResult:
    """
    accuracy, confusion matrix, classification reportë¥¼ í•œ ë²ˆì— ê³„ì‚°.

    Args:
        y_true (Iterable[int])
        y_pred (Iterable[int])
        labels (Optional[List[str]]): reportì— ì‚¬ìš©í•  í´ë˜ìŠ¤ ì´ë¦„

    Returns:
        MetricResult: accuracy, confusion matrix, report
    """
    acc = compute_accuracy(y_true, y_pred)
    cm = compute_confusion_matrix(y_true, y_pred)
    report = compute_classification_report(y_true, y_pred, target_names=labels)

    return MetricResult(
        accuracy=acc,
        confusion_matrix=cm,
        report=report,
    )
