"""
MLP(Classifier) for EuroSAT or general image classification.

- ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 32Ã—32 RGB ê¸°ì¤€ìœ¼ë¡œ flatten í›„ fully-connected MLPë¡œ ë¶„ë¥˜.
- hidden_dims, dropout, activation ë“±ì„ íŒŒë¼ë¯¸í„°ë¡œ ì„ íƒ ê°€ëŠ¥.
- TrainerTorch(trainer_torch.py)ì—ì„œ ì§ì ‘ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Literal, Optional

import torch
import torch.nn as nn

# ---------------------------------------------------------------------------
# ğŸ”µ Activation Factory
# ---------------------------------------------------------------------------


def get_activation(name: str) -> nn.Module:
    """
    ë¬¸ìì—´ ê¸°ë°˜ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ë°˜í™˜í•œë‹¤.

    Args:
        name (str): {"relu", "gelu", "tanh", "sigmoid"}

    Returns:
        nn.Module: í™œì„±í™” í•¨ìˆ˜
    """
    name = name.lower()

    if name == "relu":
        return nn.ReLU()
    if name == "gelu":
        return nn.GELU()
    if name == "tanh":
        return nn.Tanh()
    if name == "sigmoid":
        return nn.Sigmoid()

    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” activation í•¨ìˆ˜: {name}")


# ---------------------------------------------------------------------------
# ğŸ”µ Multi-Layer Perceptron (for 32x32 RGB)
# ---------------------------------------------------------------------------


@dataclass
class MLPConfig:
    """
    MLP ëª¨ë¸ êµ¬ì„± ì„¤ì • ê°’.

    Attributes:
        input_dim (int): ì…ë ¥ ë²¡í„° ì°¨ì› (ê¸°ë³¸: 32*32*3)
        num_classes (int): ë¶„ë¥˜ í´ë˜ìŠ¤ ê°œìˆ˜
        hidden_dims (List[int]): íˆë“  ë ˆì´ì–´ í¬ê¸° ë¦¬ìŠ¤íŠ¸
        activation (str): í™œì„±í™” í•¨ìˆ˜ ì¢…ë¥˜
        dropout (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
    """

    input_dim: int = 32 * 32 * 3
    num_classes: int = 10
    hidden_dims: List[int] = None
    activation: str = "relu"
    dropout: float = 0.2


class MLPClassifier(nn.Module):
    """
    ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°.

    - forwardëŠ” (B, input_dim) â†’ (B, num_classes) ì¶œë ¥.
    - hidden_dims ê¸¸ì´ì— ë”°ë¼ ë ˆì´ì–´ ê¹Šì´ ìë™ êµ¬ì„±.
    - TrainerTorchê°€ cross-entropy lossë¡œ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë¨.
    """

    def __init__(self, config: Optional[MLPConfig] = None) -> None:
        super().__init__()

        if config is None:
            config = MLPConfig(hidden_dims=[512, 256])

        self.config = config

        # í™œì„±í™” í•¨ìˆ˜ ìƒì„±
        act = get_activation(config.activation)

        layers: List[nn.Module] = []
        in_dim = config.input_dim

        # hidden layer êµ¬ì„±
        if config.hidden_dims is None or len(config.hidden_dims) == 0:
            raise ValueError("hidden_dimsëŠ” ìµœì†Œ 1ê°œ ì´ìƒ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        for h in config.hidden_dims:
            layers.append(nn.Linear(in_dim, h))
            layers.append(act)
            if config.dropout > 0:
                layers.append(nn.Dropout(config.dropout))
            in_dim = h

        # classifier layer
        layers.append(nn.Linear(in_dim, config.num_classes))

        self.net = nn.Sequential(*layers)

    # ----------------------------------------------------------------------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward ê³„ì‚°.

        Args:
            x (Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ (B, 3, 32, 32)

        Returns:
            Tensor: (B, num_classes) ë¡œì§“(logit)
        """
        # ì´ë¯¸ì§€ â†’ flatten
        x = x.view(x.size(0), -1)
        return self.net(x)

    # ----------------------------------------------------------------------
    def count_parameters(self) -> int:
        """
        í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ë°˜í™˜.

        Returns:
            int: íŒŒë¼ë¯¸í„° ìˆ˜
        """
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
